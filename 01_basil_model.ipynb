{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_basil_model.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BBBr7ViDm5-p","executionInfo":{"status":"ok","timestamp":1609783280334,"user_tz":-60,"elapsed":1720,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"f7b78302-980a-4352-ea8b-de64f70a760f"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NkiavSDTnSph","executionInfo":{"status":"ok","timestamp":1609783280336,"user_tz":-60,"elapsed":1709,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"fde8c8e8-0b59-4b3c-afb9-ba071f7090f5"},"source":["%cd gdrive/My Drive/dvl/Mask_RCNN"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/dvl/Mask_RCNN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9_A8lNBhnrd4"},"source":["#!git clone https://github.com/matterport/Mask_RCNN.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S98Z2yGh2fXv","executionInfo":{"status":"ok","timestamp":1609785970554,"user_tz":-60,"elapsed":2686076,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"ac2cdcfd-709a-4ec5-d6ef-f648122067fe"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mbuild\u001b[0m/             LICENSE              \u001b[01;34moutput\u001b[0m/           setup.cfg\n","\u001b[01;34mDEFAULT_LOGS_DIR\u001b[0m/  MANIFEST.in          README.md         setup.py\n","\u001b[01;34mdist\u001b[0m/              \u001b[01;34mmask_rcnn.egg-info\u001b[0m/  requirements.txt\n","\u001b[01;34mimages\u001b[0m/            \u001b[01;34mmrcnn\u001b[0m/               \u001b[01;34msamples\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"umac7AEhoD8U"},"source":["#! python setup.py install"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"LBRcKCq0huPK","executionInfo":{"status":"ok","timestamp":1609783281997,"user_tz":-60,"elapsed":3334,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"c960f746-c237-44fe-c0fb-396fb916a31e"},"source":["# check versions of tf and keras \r\n","%tensorflow_version 1.x\r\n","import tensorflow\r\n","print(tensorflow.__version__)\r\n","#!pip install keras==2.1.6\r\n","import keras\r\n","keras.__version__"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.1.6'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"19JSMor1oQdY"},"source":["import os\r\n","import sys\r\n","import time\r\n","import numpy as np\r\n","import imgaug  # https://github.com/aleju/imgaug (pip3 install imageaug)\r\n","import matplotlib\r\n","import matplotlib.pyplot as plt\r\n","from pycocotools.coco import COCO\r\n","from pycocotools.cocoeval import COCOeval\r\n","from pycocotools import mask as maskUtils\r\n","import zipfile\r\n","import urllib.request\r\n","import shutil\r\n","\r\n","# Root directory of the project\r\n","ROOT_DIR = os.path.abspath(\"../\")\r\n","\r\n","# Results directory\r\n","RESULTS_DIR = os.path.join(ROOT_DIR, \"results\")\r\n","\r\n","# Import Mask RCNN\r\n","sys.path.append(ROOT_DIR)  # To find local version of the library\r\n","from mrcnn.config import Config\r\n","from mrcnn import model as modellib, utils\r\n","from mrcnn import visualize\r\n","\r\n","# Path to trained weights file\r\n","COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\r\n","\r\n","# Directory to save logs and model checkpoints, if not provided\r\n","# through the command line argument --logs\r\n","DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\")\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9J1wliLrpF9R"},"source":["############################################################\r\n","#  Configurations\r\n","############################################################\r\n","\r\n","class BasilConfig(Config):\r\n","    \"\"\"Configuration for training on the nucleus segmentation dataset.\"\"\"\r\n","    # Give the configuration a recognizable name\r\n","    NAME = \"basil\"\r\n","\r\n","    # Adjust depending on your GPU memory\r\n","    IMAGES_PER_GPU = 2\r\n","\r\n","    # Number of classes (including background)\r\n","    NUM_CLASSES = 1 + 1  # Background + leaf\r\n","\r\n","    # Number of training and validation steps per epoch\r\n","    STEPS_PER_EPOCH = 75 #  train_length // batch_size\r\n","    VALIDATION_STEPS = 26 # max( val_length // batch_size)\r\n","\r\n","    # Don't exclude based on confidence. Since we have two classes\r\n","    # then 0.5 is the minimum anyway as it picks between nucleus and BG\r\n","    DETECTION_MIN_CONFIDENCE = 0.5\r\n","\r\n","    # Backbone network architecture\r\n","    # Supported values are: resnet50, resnet101\r\n","    BACKBONE = \"resnet101\" ## change for different models\r\n","\r\n","    # Select one entity (e.g. bounding boxes) out of many overlapping entities\r\n","    DETECTION_NMS_THRESHOLD = 0.3       \r\n","\r\n","    # Length of square anchor side in pixels\r\n","    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\r\n","\r\n","    # ROIs kept after non-maximum supression (training and inference)\r\n","    POST_NMS_ROIS_TRAINING = 1000\r\n","    POST_NMS_ROIS_INFERENCE = 2000\r\n","\r\n","    # Non-max suppression threshold to filter RPN proposals.\r\n","    # You can increase this during training to generate more propsals.\r\n","    RPN_NMS_THRESHOLD = 0.9\r\n","\r\n","    # How many anchors per image to use for RPN training\r\n","    RPN_TRAIN_ANCHORS_PER_IMAGE = 200  ## change for models (range 200 - 300)\r\n","\r\n","    # Image mean (RGB)\r\n","    #MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\r\n","\r\n","    # If enabled, resizes instance masks to a smaller size to reduce\r\n","    # memory load. Recommended when using high-resolution images.\r\n","    USE_MINI_MASK = True\r\n","    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\r\n","\r\n","    # Number of ROIs per image to feed to classifier/mask heads\r\n","    # The Mask RCNN paper uses 512 but often the RPN doesn't generate\r\n","    # enough positive proposals to fill this and keep a positive:negative\r\n","    # ratio of 1:3. You can increase the number of proposals by adjusting\r\n","    # the RPN NMS threshold.\r\n","    TRAIN_ROIS_PER_IMAGE = 200  ##  change for models (range 200 - 300) \r\n","\r\n","    # Maximum number of ground truth instances to use in one image\r\n","    MAX_GT_INSTANCES = 40  ##  change for models (30) \r\n","\r\n","    # Max number of final detections per image\r\n","    DETECTION_MAX_INSTANCES = 60  ##  change for models (range 200 - 300) \r\n","\r\n","\r\n","class BasilInferenceConfig(BasilConfig):\r\n","    # Set batch size to 1 to run one image at a time\r\n","    GPU_COUNT = 1\r\n","    IMAGES_PER_GPU = 1\r\n","    # Don't resize imager for inferencing\r\n","   # IMAGE_RESIZE_MODE = \"pad64\"\r\n","    # Non-max suppression threshold to filter RPN proposals.\r\n","    # You can increase this during training to generate more propsals.\r\n","    RPN_NMS_THRESHOLD = 0.7\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fslC5_-UpN_C"},"source":["############################################################\r\n","#  Dataset\r\n","############################################################\r\n","\r\n","class BasilDataset(utils.Dataset):\r\n","\r\n","    def load_basil(self, dataset_dir, subset):\r\n","        \"\"\"Load a subset of the COCO dataset.\r\n","        dataset_dir: The root directory of the COCO dataset.\r\n","        subset: What to load (train, val, test)\r\n","        class_ids: If provided, only loads images that have the given classes.\r\n","        \"\"\"\r\n","        # load json with annotated images in COCO format\r\n","        coco = COCO(\"../segments2/kailkuhn_playground/v08.json\")\r\n","\r\n","        all_image_ids = list(coco.imgs.keys())\r\n","\r\n","        # handle sets of train and val\r\n","        image_ids = []\r\n","\r\n","        if subset == \"train\": \r\n","                for image in all_image_ids:\r\n","                    if image <= len(all_image_ids)*.70:\r\n","                        image_ids.append(image)\r\n","\r\n","        elif subset == \"val\":\r\n","            for image in all_image_ids:\r\n","                if image > len(all_image_ids)*.70 and image <= len(all_image_ids)*.95:\r\n","                    image_ids.append(image)\r\n","\r\n","        elif subset == \"test\": # 11 images\r\n","            for image in all_image_ids:\r\n","                if image > len(all_image_ids)*.95:\r\n","                    image_ids.append(image)\r\n","\r\n","        \r\n","        # Load all classes or a subset?\r\n","        class_ids = sorted(coco.getCatIds())\r\n","\r\n","        # Add the class\r\n","        self.add_class(\"basil\", 1, 'leaf')\r\n","\r\n","        # Add images\r\n","        for i in image_ids:\r\n","           # print(i)\r\n","            self.add_image(\r\n","                \"basil\", image_id=i,\r\n","                path=os.path.join(\"../segments2/kailkuhn_playground/v.08\", coco.imgs[i]['file_name']),\r\n","                width=coco.imgs[i][\"width\"],\r\n","                height=coco.imgs[i][\"height\"],\r\n","                annotations=coco.loadAnns(coco.getAnnIds(\r\n","                    imgIds=[i], catIds=class_ids, iscrowd=None)))\r\n","\r\n","\r\n","\r\n","    def load_mask(self, image_id):\r\n","        \"\"\"Load instance masks for the given image.\r\n","\r\n","        Different datasets use different ways to store masks. This\r\n","        function converts the different mask format to one format\r\n","        in the form of a bitmap [height, width, instances].\r\n","\r\n","        Returns:\r\n","        masks: A bool array of shape [height, width, instance count] with\r\n","            one mask per instance.\r\n","        class_ids: a 1D array of class IDs of the instance masks.\r\n","        \"\"\"\r\n","        # If not a BASIL image, delegate to parent class.\r\n","        image_info = self.image_info[image_id]\r\n","        if image_info[\"source\"] != \"basil\":\r\n","            print('wrong source')\r\n","\r\n","        instance_masks = []\r\n","        class_ids = []\r\n","        annotations = self.image_info[image_id][\"annotations\"]\r\n","       \r\n","        # Build mask of shape [height, width, instance_count] and list\r\n","        # of class IDs that correspond to each channel of the mask.\r\n","        for annotation in annotations:\r\n","            class_id = self.map_source_class_id(\r\n","                \"basil.{}\".format(annotation['category_id'])) \r\n","            if class_id:\r\n","                m = self.annToMask(annotation, image_info[\"height\"],\r\n","                                   image_info[\"width\"])\r\n","                # Some objects are so small that they're less than 1 pixel area\r\n","                # and end up rounded out. Skip those objects.\r\n","                if m.max() < 1:\r\n","                    continue\r\n","                # Is it a crowd? If so, use a negative class ID.\r\n","                if annotation['iscrowd']:\r\n","                    # Use negative class ID for crowds\r\n","                    class_id *= -1\r\n","                    # For crowd masks, annToMask() sometimes returns a mask\r\n","                    # smaller than the given dimensions. If so, resize it.\r\n","                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\r\n","                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\r\n","                instance_masks.append(m)\r\n","                class_ids.append(class_id)\r\n","\r\n","        # Pack instance masks into an array\r\n","        if class_ids:\r\n","            mask = np.stack(instance_masks, axis=2)\r\n","            class_ids = np.array(class_ids, dtype=np.int32)\r\n","            return mask, class_ids\r\n","        else:\r\n","            # Call super class to return an empty mask\r\n","            print('calling return of parent')\r\n","            return super(BasilDataset, self).load_mask(image_id)\r\n","\r\n","\r\n","    # def image_reference(self, image_id):\r\n","    #     \"\"\"Return a link to the image in the COCO Website.\"\"\"\r\n","    #     info = self.image_info[image_id]\r\n","    #     if info[\"source\"] == \"basil\":\r\n","    #         return \"http://cocodataset.org/#explore?id={}\".format(info[\"id\"])\r\n","    #     else:\r\n","    #         super(BasilDataset, self).image_reference(image_id)\r\n","\r\n","    # The following two functions are from pycocotools with a few changes.\r\n","\r\n","    def annToRLE(self, ann, height, width):\r\n","        \"\"\"\r\n","        Convert annotation which can be polygons, uncompressed RLE to RLE.\r\n","        :return: binary mask (numpy 2D array)\r\n","        \"\"\"\r\n","        segm = ann['segmentation']\r\n","        if isinstance(segm, list):\r\n","            # polygon -- a single object might consist of multiple parts\r\n","            # we merge all parts into one mask rle code\r\n","            rles = maskUtils.frPyObjects(segm, height, width)\r\n","            rle = maskUtils.merge(rles)\r\n","        elif isinstance(segm['counts'], list):\r\n","            # uncompressed RLE\r\n","            rle = maskUtils.frPyObjects(segm, height, width)\r\n","        else:\r\n","            # rle\r\n","            rle = ann['segmentation']\r\n","        return rle\r\n","\r\n","    def annToMask(self, ann, height, width):\r\n","        \"\"\"\r\n","        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\r\n","        :return: binary mask (numpy 2D array)\r\n","        \"\"\"\r\n","        rle = self.annToRLE(ann, height, width)\r\n","        m = maskUtils.decode(rle)\r\n","        return m\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-gF8lDjp3PV"},"source":["############################################################\r\n","#  Training\r\n","############################################################\r\n","\r\n","def train(model, dataset_dir, subset):\r\n","    \"\"\"Train the model.\"\"\"\r\n","    # Training dataset.\r\n","    dataset_train = BasilDataset()\r\n","    dataset_train.load_basil(dataset_dir, subset)\r\n","    dataset_train.prepare()\r\n","\r\n","    # Validation dataset\r\n","    dataset_val = BasilDataset()\r\n","    dataset_val.load_basil(dataset_dir, \"val\")\r\n","    dataset_val.prepare()\r\n","\r\n","    # # Image augmentation\r\n","    # # http://imgaug.readthedocs.io/en/latest/source/augmenters.html\r\n","    # augmentation = iaa.SomeOf((0, 2), [\r\n","    #     iaa.Fliplr(0.5),\r\n","    #     iaa.Flipud(0.5),\r\n","    #     iaa.OneOf([iaa.Affine(rotate=90),\r\n","    #                iaa.Affine(rotate=180),\r\n","    #                iaa.Affine(rotate=270)]),\r\n","    #     iaa.Multiply((0.8, 1.5)),\r\n","    #     iaa.GaussianBlur(sigma=(0.0, 5.0))\r\n","    # ])\r\n","\r\n","    print(\"Train network heads\")\r\n","    model.train(dataset_train, dataset_val,\r\n","                learning_rate=config.LEARNING_RATE,\r\n","                epochs=20, \r\n","                # augmentation=augmentation,\r\n","                layers='heads')\r\n","\r\n","## ALL LAYERS\r\n","\r\n","    # print(\"Train all layers\")\r\n","    # model.train(dataset_train, dataset_val,\r\n","    #             learning_rate=config.LEARNING_RATE,\r\n","    #             epochs=40,\r\n","    #             # augmentation=augmentation,\r\n","    #             layers='all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eNwPOk6TRdbw"},"source":["############################################################\r\n","#  RLE Encoding\r\n","############################################################\r\n","\r\n","def rle_encode(mask):\r\n","    \"\"\"Encodes a mask in Run Length Encoding (RLE).\r\n","    Returns a string of space-separated values.\r\n","    \"\"\"\r\n","    assert mask.ndim == 2, \"Mask must be of shape [Height, Width]\"\r\n","    # Flatten it column wise\r\n","    m = mask.T.flatten()\r\n","    # Compute gradient. Equals 1 or -1 at transition points\r\n","    g = np.diff(np.concatenate([[0], m, [0]]), n=1)\r\n","    # 1-based indicies of transition points (where gradient != 0)\r\n","    rle = np.where(g != 0)[0].reshape([-1, 2]) + 1\r\n","    # Convert second index in each pair to lenth\r\n","    rle[:, 1] = rle[:, 1] - rle[:, 0]\r\n","    return \" \".join(map(str, rle.flatten()))\r\n","\r\n","\r\n","def rle_decode(rle, shape):\r\n","    \"\"\"Decodes an RLE encoded list of space separated\r\n","    numbers and returns a binary mask.\"\"\"\r\n","    rle = list(map(int, rle.split()))\r\n","    rle = np.array(rle, dtype=np.int32).reshape([-1, 2])\r\n","    rle[:, 1] += rle[:, 0]\r\n","    rle -= 1\r\n","    mask = np.zeros([shape[0] * shape[1]], np.bool)\r\n","    for s, e in rle:\r\n","        assert 0 <= s < mask.shape[0]\r\n","        assert 1 <= e <= mask.shape[0], \"shape: {}  s {}  e {}\".format(shape, s, e)\r\n","        mask[s:e] = 1\r\n","    # Reshape and transpose\r\n","    mask = mask.reshape([shape[1], shape[0]]).T\r\n","    return mask\r\n","\r\n","\r\n","def mask_to_rle(image_id, mask, scores):\r\n","    \"Encodes instance masks to submission format.\"\r\n","    assert mask.ndim == 3, \"Mask must be [H, W, count]\"\r\n","    # If mask is empty, return line with image ID only\r\n","    if mask.shape[-1] == 0:\r\n","        return \"{},\".format(image_id)\r\n","    # Remove mask overlaps\r\n","    # Multiply each instance mask by its score order\r\n","    # then take the maximum across the last dimension\r\n","    order = np.argsort(scores)[::-1] + 1  # 1-based descending\r\n","    mask = np.max(mask * np.reshape(order, [1, 1, -1]), -1)\r\n","    # Loop over instance masks\r\n","    lines = []\r\n","    for o in order:\r\n","        m = np.where(mask == o, 1, 0)\r\n","        # Skip if empty\r\n","        if m.sum() == 0.0:\r\n","            continue\r\n","        rle = rle_encode(m)\r\n","        lines.append(\"{}, {}\".format(image_id, rle))\r\n","    return \"\\n\".join(lines)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QE0_dcCEfjUk","executionInfo":{"status":"ok","timestamp":1609783286762,"user_tz":-60,"elapsed":8041,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"37a09ff1-b0e5-40e7-c453-ce0c695cd986"},"source":["############################################################\r\n","#  TRAIN START\r\n","############################################################\r\n","\r\n","# Configurations\r\n","config = BasilConfig()\r\n","config.display()\r\n","\r\n","# Create model\r\n","model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir= 'DEFAULT_LOGS_DIR')\r\n","\r\n","\r\n","# Select weights file to load\r\n","weights_path = COCO_MODEL_PATH\r\n","\r\n","#weights_path = model.find_last()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Configurations:\n","BACKBONE                       resnet101\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     2\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DETECTION_MAX_INSTANCES        60\n","DETECTION_MIN_CONFIDENCE       0.5\n","DETECTION_NMS_THRESHOLD        0.3\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 2\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  1024\n","IMAGE_META_SIZE                14\n","IMAGE_MIN_DIM                  800\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [1024 1024    3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.001\n","LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               40\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (56, 56)\n","NAME                           basil\n","NUM_CLASSES                    2\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        2000\n","POST_NMS_ROIS_TRAINING         1000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.9\n","RPN_TRAIN_ANCHORS_PER_IMAGE    200\n","STEPS_PER_EPOCH                75\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           200\n","USE_MINI_MASK                  True\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               26\n","WEIGHT_DECAY                   0.0001\n","\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1944: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:542: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:589: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BLFbrson7NFS","executionInfo":{"status":"ok","timestamp":1609783292234,"user_tz":-60,"elapsed":13504,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"1e416002-0200-4e57-9208-2490402e3b16"},"source":["# Load weights\r\n","print(\"Loading weights \", weights_path) \r\n","\r\n","# load call\r\n","model.load_weights(weights_path, by_name=True, exclude=[\r\n","    \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\r\n","    \"mrcnn_bbox\", \"mrcnn_mask\"])# Exclude the last layers because they require a matching number of classes\r\n","\r\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading weights  /content/gdrive/My Drive/dvl/mask_rcnn_coco.h5\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:180: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:184: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:193: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:200: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Oxf8k0ZFIqjP","executionInfo":{"status":"ok","timestamp":1609785970551,"user_tz":-60,"elapsed":2691812,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"e43eb8da-a778-437b-8960-fecf134d41dc"},"source":["# Train CALL\r\n","train(model, os.path.join(ROOT_DIR, 'segments2/kailkuhn_playground/v.08'), 'train')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.02s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.01s)\n","creating index...\n","index created!\n","Train network heads\n","\n","Starting at epoch 0. LR=0.001\n","\n","Checkpoint Path: DEFAULT_LOGS_DIR/basil20210104T1801/mask_rcnn_basil_{epoch:04d}.h5\n","Selecting layers to train\n","fpn_c5p5               (Conv2D)\n","fpn_c4p4               (Conv2D)\n","fpn_c3p3               (Conv2D)\n","fpn_c2p2               (Conv2D)\n","fpn_p5                 (Conv2D)\n","fpn_p2                 (Conv2D)\n","fpn_p3                 (Conv2D)\n","fpn_p4                 (Conv2D)\n","In model:  rpn_model\n","    rpn_conv_shared        (Conv2D)\n","    rpn_class_raw          (Conv2D)\n","    rpn_bbox_pred          (Conv2D)\n","mrcnn_mask_conv1       (TimeDistributed)\n","mrcnn_mask_bn1         (TimeDistributed)\n","mrcnn_mask_conv2       (TimeDistributed)\n","mrcnn_mask_bn2         (TimeDistributed)\n","mrcnn_class_conv1      (TimeDistributed)\n","mrcnn_class_bn1        (TimeDistributed)\n","mrcnn_mask_conv3       (TimeDistributed)\n","mrcnn_mask_bn3         (TimeDistributed)\n","mrcnn_class_conv2      (TimeDistributed)\n","mrcnn_class_bn2        (TimeDistributed)\n","mrcnn_mask_conv4       (TimeDistributed)\n","mrcnn_mask_bn4         (TimeDistributed)\n","mrcnn_bbox_fc          (TimeDistributed)\n","mrcnn_mask_deconv      (TimeDistributed)\n","mrcnn_class_logits     (TimeDistributed)\n","mrcnn_mask             (TimeDistributed)\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/tensorflow-1.15.2/python3.6/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:977: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:964: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n","  UserWarning('Using a generator with `use_multiprocessing=True`'\n"],"name":"stderr"},{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:783: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:786: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n","\n","Epoch 1/20\n","74/75 [============================>.] - ETA: 1s - loss: 2.3535 - rpn_class_loss: 0.1085 - rpn_bbox_loss: 0.8480 - mrcnn_class_loss: 0.4413 - mrcnn_bbox_loss: 0.5269 - mrcnn_mask_loss: 0.4287"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:2348: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n","  UserWarning('Using a generator with `use_multiprocessing=True`'\n"],"name":"stderr"},{"output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r75/75 [==============================] - 187s 2s/step - loss: 2.3404 - rpn_class_loss: 0.1076 - rpn_bbox_loss: 0.8425 - mrcnn_class_loss: 0.4395 - mrcnn_bbox_loss: 0.5243 - mrcnn_mask_loss: 0.4265 - val_loss: 1.4243 - val_rpn_class_loss: 0.0363 - val_rpn_bbox_loss: 0.4871 - val_mrcnn_class_loss: 0.3420 - val_mrcnn_bbox_loss: 0.3133 - val_mrcnn_mask_loss: 0.2457\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:869: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n","\n","Epoch 2/20\n","75/75 [==============================] - 124s 2s/step - loss: 1.3601 - rpn_class_loss: 0.0481 - rpn_bbox_loss: 0.4964 - mrcnn_class_loss: 0.3183 - mrcnn_bbox_loss: 0.2748 - mrcnn_mask_loss: 0.2226 - val_loss: 1.3131 - val_rpn_class_loss: 0.0363 - val_rpn_bbox_loss: 0.4318 - val_mrcnn_class_loss: 0.3443 - val_mrcnn_bbox_loss: 0.2753 - val_mrcnn_mask_loss: 0.2254\n","Epoch 3/20\n","75/75 [==============================] - 124s 2s/step - loss: 1.1853 - rpn_class_loss: 0.0393 - rpn_bbox_loss: 0.4209 - mrcnn_class_loss: 0.2883 - mrcnn_bbox_loss: 0.2344 - mrcnn_mask_loss: 0.2025 - val_loss: 1.2136 - val_rpn_class_loss: 0.0331 - val_rpn_bbox_loss: 0.4017 - val_mrcnn_class_loss: 0.3157 - val_mrcnn_bbox_loss: 0.2515 - val_mrcnn_mask_loss: 0.2116\n","Epoch 4/20\n","75/75 [==============================] - 125s 2s/step - loss: 1.1021 - rpn_class_loss: 0.0355 - rpn_bbox_loss: 0.3800 - mrcnn_class_loss: 0.2887 - mrcnn_bbox_loss: 0.2097 - mrcnn_mask_loss: 0.1882 - val_loss: 1.1383 - val_rpn_class_loss: 0.0266 - val_rpn_bbox_loss: 0.3861 - val_mrcnn_class_loss: 0.3007 - val_mrcnn_bbox_loss: 0.2277 - val_mrcnn_mask_loss: 0.1973\n","Epoch 5/20\n","75/75 [==============================] - 127s 2s/step - loss: 1.0375 - rpn_class_loss: 0.0310 - rpn_bbox_loss: 0.3661 - mrcnn_class_loss: 0.2755 - mrcnn_bbox_loss: 0.1887 - mrcnn_mask_loss: 0.1762 - val_loss: 1.0932 - val_rpn_class_loss: 0.0285 - val_rpn_bbox_loss: 0.3651 - val_mrcnn_class_loss: 0.3001 - val_mrcnn_bbox_loss: 0.2185 - val_mrcnn_mask_loss: 0.1809\n","Epoch 6/20\n","75/75 [==============================] - 123s 2s/step - loss: 0.9968 - rpn_class_loss: 0.0331 - rpn_bbox_loss: 0.3245 - mrcnn_class_loss: 0.2672 - mrcnn_bbox_loss: 0.1875 - mrcnn_mask_loss: 0.1845 - val_loss: 1.1061 - val_rpn_class_loss: 0.0276 - val_rpn_bbox_loss: 0.3492 - val_mrcnn_class_loss: 0.3151 - val_mrcnn_bbox_loss: 0.2239 - val_mrcnn_mask_loss: 0.1904\n","Epoch 7/20\n","75/75 [==============================] - 124s 2s/step - loss: 0.9253 - rpn_class_loss: 0.0291 - rpn_bbox_loss: 0.3070 - mrcnn_class_loss: 0.2581 - mrcnn_bbox_loss: 0.1632 - mrcnn_mask_loss: 0.1679 - val_loss: 1.0413 - val_rpn_class_loss: 0.0238 - val_rpn_bbox_loss: 0.3475 - val_mrcnn_class_loss: 0.2926 - val_mrcnn_bbox_loss: 0.1975 - val_mrcnn_mask_loss: 0.1798\n","Epoch 8/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.8941 - rpn_class_loss: 0.0274 - rpn_bbox_loss: 0.2914 - mrcnn_class_loss: 0.2478 - mrcnn_bbox_loss: 0.1595 - mrcnn_mask_loss: 0.1679 - val_loss: 1.0594 - val_rpn_class_loss: 0.0221 - val_rpn_bbox_loss: 0.3459 - val_mrcnn_class_loss: 0.2957 - val_mrcnn_bbox_loss: 0.2082 - val_mrcnn_mask_loss: 0.1875\n","Epoch 9/20\n","75/75 [==============================] - 124s 2s/step - loss: 0.8640 - rpn_class_loss: 0.0257 - rpn_bbox_loss: 0.2908 - mrcnn_class_loss: 0.2358 - mrcnn_bbox_loss: 0.1476 - mrcnn_mask_loss: 0.1640 - val_loss: 0.9932 - val_rpn_class_loss: 0.0228 - val_rpn_bbox_loss: 0.3148 - val_mrcnn_class_loss: 0.2911 - val_mrcnn_bbox_loss: 0.1874 - val_mrcnn_mask_loss: 0.1770\n","Epoch 10/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.8254 - rpn_class_loss: 0.0231 - rpn_bbox_loss: 0.2728 - mrcnn_class_loss: 0.2325 - mrcnn_bbox_loss: 0.1406 - mrcnn_mask_loss: 0.1564 - val_loss: 0.9862 - val_rpn_class_loss: 0.0246 - val_rpn_bbox_loss: 0.3188 - val_mrcnn_class_loss: 0.2771 - val_mrcnn_bbox_loss: 0.1862 - val_mrcnn_mask_loss: 0.1794\n","Epoch 11/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.7817 - rpn_class_loss: 0.0241 - rpn_bbox_loss: 0.2477 - mrcnn_class_loss: 0.2225 - mrcnn_bbox_loss: 0.1368 - mrcnn_mask_loss: 0.1506 - val_loss: 1.0067 - val_rpn_class_loss: 0.0246 - val_rpn_bbox_loss: 0.3136 - val_mrcnn_class_loss: 0.3051 - val_mrcnn_bbox_loss: 0.1915 - val_mrcnn_mask_loss: 0.1719\n","Epoch 12/20\n","75/75 [==============================] - 124s 2s/step - loss: 0.7710 - rpn_class_loss: 0.0237 - rpn_bbox_loss: 0.2475 - mrcnn_class_loss: 0.2193 - mrcnn_bbox_loss: 0.1291 - mrcnn_mask_loss: 0.1513 - val_loss: 0.9668 - val_rpn_class_loss: 0.0228 - val_rpn_bbox_loss: 0.2996 - val_mrcnn_class_loss: 0.2698 - val_mrcnn_bbox_loss: 0.1892 - val_mrcnn_mask_loss: 0.1855\n","Epoch 13/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.7235 - rpn_class_loss: 0.0222 - rpn_bbox_loss: 0.2173 - mrcnn_class_loss: 0.2135 - mrcnn_bbox_loss: 0.1208 - mrcnn_mask_loss: 0.1497 - val_loss: 0.9722 - val_rpn_class_loss: 0.0195 - val_rpn_bbox_loss: 0.2986 - val_mrcnn_class_loss: 0.2836 - val_mrcnn_bbox_loss: 0.1907 - val_mrcnn_mask_loss: 0.1799\n","Epoch 14/20\n","75/75 [==============================] - 126s 2s/step - loss: 0.7299 - rpn_class_loss: 0.0227 - rpn_bbox_loss: 0.2329 - mrcnn_class_loss: 0.2066 - mrcnn_bbox_loss: 0.1197 - mrcnn_mask_loss: 0.1479 - val_loss: 0.9684 - val_rpn_class_loss: 0.0214 - val_rpn_bbox_loss: 0.3216 - val_mrcnn_class_loss: 0.2740 - val_mrcnn_bbox_loss: 0.1837 - val_mrcnn_mask_loss: 0.1677\n","Epoch 15/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.7165 - rpn_class_loss: 0.0220 - rpn_bbox_loss: 0.2252 - mrcnn_class_loss: 0.2055 - mrcnn_bbox_loss: 0.1168 - mrcnn_mask_loss: 0.1469 - val_loss: 0.9111 - val_rpn_class_loss: 0.0217 - val_rpn_bbox_loss: 0.2772 - val_mrcnn_class_loss: 0.2635 - val_mrcnn_bbox_loss: 0.1762 - val_mrcnn_mask_loss: 0.1725\n","Epoch 16/20\n","75/75 [==============================] - 123s 2s/step - loss: 0.6564 - rpn_class_loss: 0.0201 - rpn_bbox_loss: 0.1881 - mrcnn_class_loss: 0.2007 - mrcnn_bbox_loss: 0.1088 - mrcnn_mask_loss: 0.1387 - val_loss: 0.9274 - val_rpn_class_loss: 0.0199 - val_rpn_bbox_loss: 0.2832 - val_mrcnn_class_loss: 0.2639 - val_mrcnn_bbox_loss: 0.1769 - val_mrcnn_mask_loss: 0.1836\n","Epoch 17/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.6349 - rpn_class_loss: 0.0182 - rpn_bbox_loss: 0.1758 - mrcnn_class_loss: 0.1926 - mrcnn_bbox_loss: 0.1058 - mrcnn_mask_loss: 0.1425 - val_loss: 0.9447 - val_rpn_class_loss: 0.0188 - val_rpn_bbox_loss: 0.3016 - val_mrcnn_class_loss: 0.2773 - val_mrcnn_bbox_loss: 0.1795 - val_mrcnn_mask_loss: 0.1674\n","Epoch 18/20\n","75/75 [==============================] - 124s 2s/step - loss: 0.6337 - rpn_class_loss: 0.0205 - rpn_bbox_loss: 0.1944 - mrcnn_class_loss: 0.1854 - mrcnn_bbox_loss: 0.0979 - mrcnn_mask_loss: 0.1355 - val_loss: 0.9364 - val_rpn_class_loss: 0.0200 - val_rpn_bbox_loss: 0.2955 - val_mrcnn_class_loss: 0.2817 - val_mrcnn_bbox_loss: 0.1767 - val_mrcnn_mask_loss: 0.1626\n","Epoch 19/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.6290 - rpn_class_loss: 0.0194 - rpn_bbox_loss: 0.1900 - mrcnn_class_loss: 0.1830 - mrcnn_bbox_loss: 0.0955 - mrcnn_mask_loss: 0.1411 - val_loss: 0.9372 - val_rpn_class_loss: 0.0215 - val_rpn_bbox_loss: 0.2980 - val_mrcnn_class_loss: 0.2753 - val_mrcnn_bbox_loss: 0.1753 - val_mrcnn_mask_loss: 0.1671\n","Epoch 20/20\n","75/75 [==============================] - 125s 2s/step - loss: 0.5994 - rpn_class_loss: 0.0189 - rpn_bbox_loss: 0.1736 - mrcnn_class_loss: 0.1796 - mrcnn_bbox_loss: 0.0930 - mrcnn_mask_loss: 0.1343 - val_loss: 1.0065 - val_rpn_class_loss: 0.0208 - val_rpn_bbox_loss: 0.3223 - val_mrcnn_class_loss: 0.3010 - val_mrcnn_bbox_loss: 0.1876 - val_mrcnn_mask_loss: 0.1747\n"],"name":"stdout"}]}]}