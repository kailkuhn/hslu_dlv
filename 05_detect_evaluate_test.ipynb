{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Evaluation_TEST.ipynb","provenance":[{"file_id":"1GYQOwxkKXR8xswjPKABIrqLCInjqvSkL","timestamp":1609429843937}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"99YbWi3V-biO","executionInfo":{"status":"ok","timestamp":1609967485816,"user_tz":-60,"elapsed":1965,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"34b96819-e9c4-41bd-eebe-824e10a11a67"},"source":["from google.colab import drive\r\n","drive.mount('/content/gdrive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CCghKSP0s3Nz","executionInfo":{"status":"ok","timestamp":1609967485817,"user_tz":-60,"elapsed":1954,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"19b440ec-c5db-4490-88a2-f2e2f8195d9d"},"source":["%cd gdrive/My Drive/dvl/Mask_RCNN"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/dvl/Mask_RCNN\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":88},"id":"lfe_jCbkEGMF","executionInfo":{"status":"ok","timestamp":1609967487690,"user_tz":-60,"elapsed":3815,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"c7b63942-9799-45cf-e824-b88aa07c22d0"},"source":["# check versions of tf and keras\r\n","%tensorflow_version 1.x\r\n","import tensorflow\r\n","print(tensorflow.__version__)\r\n","#!pip install keras==2.1.6\r\n","import keras\r\n","keras.__version__"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n","1.15.2\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.3.1'"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"rzSHBeNJ8Gk_"},"source":["import os\r\n","import sys\r\n","import random\r\n","import math\r\n","import re\r\n","import time\r\n","import numpy as np\r\n","import tensorflow as tf\r\n","import matplotlib\r\n","import matplotlib.pyplot as plt\r\n","import matplotlib.patches as patches\r\n","\r\n","# Root directory of the project\r\n","ROOT_DIR = os.path.abspath(\"../\")\r\n","\r\n","# Import Mask RCNN\r\n","sys.path.append(ROOT_DIR)  # To find local version of the library\r\n","from mrcnn import utils\r\n","from mrcnn import visualize\r\n","from mrcnn.visualize import display_images\r\n","import mrcnn.model as modellib\r\n","from mrcnn.model import log\r\n","from mrcnn.config import Config\r\n","\r\n","from pycocotools.coco import COCO\r\n","from pycocotools import mask as maskUtils\r\n","\r\n","%matplotlib inline \r\n","\r\n","# Directory to save logs and trained model\r\n","MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NFJg1EuPBSk9"},"source":["\r\n","\r\n","```\r\n","# Als Code formatiert\r\n","```\r\n","\r\n","#**Configurations**"]},{"cell_type":"code","metadata":{"id":"s2Co7lfEAj26"},"source":["############################################################\r\n","#  Configurations\r\n","############################################################\r\n","\r\n","class BasilConfig(Config):\r\n","    \"\"\"Configuration for training on the nucleus segmentation dataset.\"\"\"\r\n","    # Give the configuration a recognizable name\r\n","    NAME = \"basil\"\r\n","\r\n","    # Adjust depending on your GPU memory\r\n","    IMAGES_PER_GPU = 2\r\n","\r\n","    # Number of classes (including background)\r\n","    NUM_CLASSES = 1 + 1  # Background + leaf\r\n","\r\n","    # Number of training and validation steps per epoch\r\n","    STEPS_PER_EPOCH = 75 \r\n","    VALIDATION_STEPS = 26 \r\n","\r\n","    # Don't exclude based on confidence. Since we have two classes\r\n","    # then 0.5 is the minimum anyway as it picks between nucleus and BG\r\n","    DETECTION_MIN_CONFIDENCE = 0.5\r\n","\r\n","    # Backbone network architecture\r\n","    # Supported values are: resnet50, resnet101\r\n","    BACKBONE = \"resnet101\" ## change for different models\r\n","\r\n","    # Select one entity (e.g. bounding boxes) out of many overlapping entities\r\n","    DETECTION_NMS_THRESHOLD = 0.3       \r\n","\r\n","    # Length of square anchor side in pixels\r\n","    RPN_ANCHOR_SCALES = (16, 32, 64, 128, 256)\r\n","\r\n","    # ROIs kept after non-maximum supression (training and inference)\r\n","    POST_NMS_ROIS_TRAINING = 1000\r\n","    POST_NMS_ROIS_INFERENCE = 2000\r\n","\r\n","    # Non-max suppression threshold to filter RPN proposals.\r\n","    # You can increase this during training to generate more propsals.\r\n","    RPN_NMS_THRESHOLD = 0.9\r\n","\r\n","    # How many anchors per image to use for RPN training\r\n","    RPN_TRAIN_ANCHORS_PER_IMAGE = 300  ## change for models (range 200 - 300)\r\n","\r\n","    # Image mean (RGB)\r\n","    #MEAN_PIXEL = np.array([43.53, 39.56, 48.22])\r\n","\r\n","    # If enabled, resizes instance masks to a smaller size to reduce\r\n","    # memory load. Recommended when using high-resolution images.\r\n","    USE_MINI_MASK = True\r\n","    MINI_MASK_SHAPE = (56, 56)  # (height, width) of the mini-mask\r\n","\r\n","    # Number of ROIs per image to feed to classifier/mask heads\r\n","    TRAIN_ROIS_PER_IMAGE = 300  ##  change for models (range 200 - 300) \r\n","\r\n","    # Maximum number of ground truth instances to use in one image\r\n","    MAX_GT_INSTANCES = 40  ##  change for models (30) \r\n","\r\n","    # Max number of final detections per image\r\n","    DETECTION_MAX_INSTANCES = 60  ##  change for models (range 200 - 300) \r\n","\r\n","\r\n","class BasilInferenceConfig(BasilConfig):\r\n","    # Set batch size to 1 to run one image at a time\r\n","    GPU_COUNT = 1\r\n","    IMAGES_PER_GPU = 1\r\n","    # Don't resize imager for inferencing\r\n","   # IMAGE_RESIZE_MODE = \"pad64\"\r\n","    # Non-max suppression threshold to filter RPN proposals.\r\n","    # You can increase this during training to generate more propsals.\r\n","    RPN_NMS_THRESHOLD = 0.7\r\n","\r\n","### CURRENT MODEL ####\r\n","current_model = 'model_9'\r\n","\r\n","# Path to a specific weights file\r\n","weights_path = os.path.join(ROOT_DIR, \"Mask_RCNN/DEFAULT_LOGS_DIR/Model_9/mask_rcnn_basil_0015.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pbrBYL4hBETo"},"source":["############################################################\r\n","#  Dataset\r\n","############################################################\r\n","\r\n","class BasilDataset(utils.Dataset):\r\n","\r\n","    def load_basil(self, dataset_dir, subset):\r\n","        \"\"\"Load a subset of the COCO dataset.\r\n","        dataset_dir: The root directory of the COCO dataset.\r\n","        subset: What to load (train, val, test)\r\n","        class_ids: If provided, only loads images that have the given classes.\r\n","        \"\"\"\r\n","\r\n","        # load json with annotated images in COCO format\r\n","        coco = COCO(\"../segments2/kailkuhn_playground/v08.json\")\r\n","\r\n","        all_image_ids = list(coco.imgs.keys())\r\n","\r\n","        # handle sets of train and val\r\n","        image_ids = []\r\n","\r\n","        if subset == \"train\": #1-151\r\n","                for image in all_image_ids:\r\n","                    if image <= len(all_image_ids)*.70:\r\n","                        image_ids.append(image)\r\n","\r\n","        elif subset == \"val\": #152-205\r\n","            for image in all_image_ids:\r\n","                if image > len(all_image_ids)*.70 and image <= len(all_image_ids)*.95:\r\n","                    image_ids.append(image)\r\n","\r\n","        elif subset == \"test\": #206-216\r\n","            for image in all_image_ids:\r\n","                if image > len(all_image_ids)*.95:\r\n","                    image_ids.append(image)\r\n","\r\n","        \r\n","        # Load all classes or a subset?\r\n","        class_ids = sorted(coco.getCatIds())\r\n","\r\n","        # Add the class\r\n","        self.add_class(\"basil\", 1, 'leaf')\r\n","\r\n","        # Add images\r\n","        for i in image_ids:\r\n","           # print(i)\r\n","            self.add_image(\r\n","                \"basil\", image_id=i,\r\n","                path=os.path.join(\"../segments2/kailkuhn_playground/v.08\", coco.imgs[i]['file_name']),\r\n","                width=coco.imgs[i][\"width\"],\r\n","                height=coco.imgs[i][\"height\"],\r\n","                annotations=coco.loadAnns(coco.getAnnIds(\r\n","                    imgIds=[i], catIds=class_ids, iscrowd=None)))\r\n","\r\n","\r\n","\r\n","    def load_mask(self, image_id):\r\n","        \"\"\"Load instance masks for the given image.\r\n","\r\n","        Different datasets use different ways to store masks. This\r\n","        function converts the different mask format to one format\r\n","        in the form of a bitmap [height, width, instances].\r\n","\r\n","        Returns:\r\n","        masks: A bool array of shape [height, width, instance count] with\r\n","            one mask per instance.\r\n","        class_ids: a 1D array of class IDs of the instance masks.\r\n","        \"\"\"\r\n","        # If not a COCO image, delegate to parent class.\r\n","        image_info = self.image_info[image_id]\r\n","        if image_info[\"source\"] != \"basil\":\r\n","            print('wrong source')\r\n","\r\n","        instance_masks = []\r\n","        class_ids = []\r\n","  #      print(self.image_info[image_id])\r\n","  #      print(self.image_info[image_id][\"annotations\"])\r\n","        annotations = self.image_info[image_id][\"annotations\"]\r\n","        # Build mask of shape [height, width, instance_count] and list\r\n","        # of class IDs that correspond to each channel of the mask.\r\n","        for annotation in annotations:\r\n","           # print( 'category if line 71' + str(annotation['category_id']))  # just hardcode 0?\r\n","            class_id = self.map_source_class_id(\r\n","                \"basil.{}\".format(annotation['category_id'])) \r\n","            if class_id:\r\n","                m = self.annToMask(annotation, image_info[\"height\"],\r\n","                                   image_info[\"width\"])\r\n","                # Some objects are so small that they're less than 1 pixel area\r\n","                # and end up rounded out. Skip those objects.\r\n","                if m.max() < 1:\r\n","                    continue\r\n","                # Is it a crowd? If so, use a negative class ID.\r\n","                if annotation['iscrowd']:\r\n","                    # Use negative class ID for crowds\r\n","                    class_id *= -1\r\n","                    # For crowd masks, annToMask() sometimes returns a mask\r\n","                    # smaller than the given dimensions. If so, resize it.\r\n","                    if m.shape[0] != image_info[\"height\"] or m.shape[1] != image_info[\"width\"]:\r\n","                        m = np.ones([image_info[\"height\"], image_info[\"width\"]], dtype=bool)\r\n","                instance_masks.append(m)\r\n","                class_ids.append(class_id)\r\n","\r\n","        # Pack instance masks into an array\r\n","        if class_ids:\r\n","            mask = np.stack(instance_masks, axis=2)\r\n","            class_ids = np.array(class_ids, dtype=np.int32)\r\n","            return mask, class_ids\r\n","        else:\r\n","            # Call super class to return an empty mask\r\n","            print('calling return of parent')\r\n","            return super(BasilDataset, self).load_mask(image_id)\r\n","\r\n","\r\n","    # The following two functions are from pycocotools with a few changes.\r\n","\r\n","    def annToRLE(self, ann, height, width):\r\n","        \"\"\"\r\n","        Convert annotation which can be polygons, uncompressed RLE to RLE.\r\n","        :return: binary mask (numpy 2D array)\r\n","        \"\"\"\r\n","        segm = ann['segmentation']\r\n","        if isinstance(segm, list):\r\n","            # polygon -- a single object might consist of multiple parts\r\n","            # we merge all parts into one mask rle code\r\n","            rles = maskUtils.frPyObjects(segm, height, width)\r\n","            rle = maskUtils.merge(rles)\r\n","        elif isinstance(segm['counts'], list):\r\n","            # uncompressed RLE\r\n","            rle = maskUtils.frPyObjects(segm, height, width)\r\n","        else:\r\n","            # rle\r\n","            rle = ann['segmentation']\r\n","        return rle\r\n","\r\n","    def annToMask(self, ann, height, width):\r\n","        \"\"\"\r\n","        Convert annotation which can be polygons, uncompressed RLE, or RLE to binary mask.\r\n","        :return: binary mask (numpy 2D array)\r\n","        \"\"\"\r\n","        rle = self.annToRLE(ann, height, width)\r\n","        m = maskUtils.decode(rle)\r\n","        return m\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"00DAEprFBqt8"},"source":["#**Set Configurations**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uxL9vWnL9FX_","executionInfo":{"status":"ok","timestamp":1609967488192,"user_tz":-60,"elapsed":4290,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"fa43ab99-b6af-489a-fd3e-48e258078ac6"},"source":["# Dataset directory\r\n","DATASET_DIR = os.path.join(ROOT_DIR, 'segments2/kailkuhn_playground/v.08')\r\n","\r\n","# Inference Configuration\r\n","config = BasilInferenceConfig()\r\n","config.display()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","Configurations:\n","BACKBONE                       resnet101\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     1\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DETECTION_MAX_INSTANCES        60\n","DETECTION_MIN_CONFIDENCE       0.5\n","DETECTION_NMS_THRESHOLD        0.3\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 1\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  1024\n","IMAGE_META_SIZE                14\n","IMAGE_MIN_DIM                  800\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [1024 1024    3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.001\n","LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               40\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (56, 56)\n","NAME                           basil\n","NUM_CLASSES                    2\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        2000\n","POST_NMS_ROIS_TRAINING         1000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (16, 32, 64, 128, 256)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.7\n","RPN_TRAIN_ANCHORS_PER_IMAGE    300\n","STEPS_PER_EPOCH                75\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           300\n","USE_MINI_MASK                  True\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               26\n","WEIGHT_DECAY                   0.0001\n","\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"pGAzRwkQBw1D"},"source":["# **Notebook Preferences**"]},{"cell_type":"code","metadata":{"id":"S1w5tZ2RBMzM"},"source":["def get_ax(rows=1, cols=1, size=16):\r\n","    \"\"\"Return a Matplotlib Axes array to be used in\r\n","    all visualizations in the notebook. Provide a\r\n","    central point to control graph sizes.\r\n","    \r\n","    Adjust the size attribute to control how big to render images\r\n","    \"\"\"\r\n","    fig, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\r\n","    fig.tight_layout()\r\n","    return ax"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oe7F7_yPB7wW"},"source":["# **Load Validation Dataset**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M0_t65tDBQoV","executionInfo":{"status":"ok","timestamp":1609967488193,"user_tz":-60,"elapsed":4277,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"f065da5d-61ee-4037-c838-885a3dc7da7c"},"source":["# Load validation dataset\r\n","dataset = BasilDataset()\r\n","dataset.load_basil(DATASET_DIR, \"test\")\r\n","dataset.prepare()\r\n","\r\n","print(\"Images: {}\\nClasses: {}\".format(len(dataset.image_ids), dataset.class_names))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.04s)\n","creating index...\n","index created!\n","Images: 11\n","Classes: ['BG', 'leaf']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"jvvwcH8sD0Q3"},"source":["# **Load Model**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e7tZ0jEeDI-F","executionInfo":{"status":"ok","timestamp":1609967494258,"user_tz":-60,"elapsed":10334,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"4f9d5346-6aee-46c8-f160-4c83902945f0"},"source":["model = modellib.MaskRCNN(mode=\"inference\",\r\n","                              model_dir= 'DEFAULT_LOGS_DIR',\r\n","                              config=config)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n","Instructions for updating:\n","If using Keras pass *_constraint arguments to layers.\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:336: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:391: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:415: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:704: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:706: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n","\n","WARNING:tensorflow:From /content/gdrive/My Drive/dvl/Mask_RCNN/mrcnn/model.py:756: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D-VA4IFaD3xr","executionInfo":{"status":"ok","timestamp":1609967503374,"user_tz":-60,"elapsed":19442,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"70a2a1ef-f60c-4899-a7b7-a5997a46af1f"},"source":["\r\n","\r\n","# Load weights\r\n","print(\"Loading weights \", weights_path)\r\n","model.load_weights(weights_path, by_name=True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Loading weights  /content/gdrive/My Drive/dvl/Mask_RCNN/DEFAULT_LOGS_DIR/Model_9/mask_rcnn_basil_0015.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YsMO10lA0PcS"},"source":["############################################################\r\n","#  RUN DETECTION and visualize\r\n","############################################################\r\n","\r\n","\r\n","######################\r\n","image_ids = dataset.image_ids\r\n","\r\n","for image_id in image_ids:\r\n","\r\n","    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\r\n","        modellib.load_image_gt(dataset, config, image_id, use_mini_mask=True)\r\n","\r\n","\r\n","    info = dataset.image_info[image_id]\r\n","\r\n","    print(\"image ID: {}.{} ({}) {}\".format(info[\"source\"], info[\"id\"], image_id, dataset.image_reference(image_id))) # image reference is not used\r\n","\r\n","\r\n","    # Run object detection\r\n","    results = model.detect([image], verbose=1)\r\n","\r\n","    # Display results\r\n","    ax = get_ax(1)\r\n","    r = results[0]\r\n","    visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \r\n","                                dataset.class_names, r['scores'], ax=ax,\r\n","                                title=\"Predictions\")\r\n","\r\n","    plt.savefig(f'{image_id}_{current_model}.png')\r\n","\r\n","    log(\"gt_class_id\", gt_class_id)\r\n","    log(\"gt_bbox\", gt_bbox)\r\n","    log(\"gt_mask\", gt_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5GyRZVFf7Dd"},"source":["#######################\r\n","#### Compute IoU   ####\r\n","#######################\r\n","\r\n","import statistics \r\n","# # ##### results  Kail\r\n","image_ids = dataset.image_ids \r\n","\r\n","\r\n","config.BATCH_SIZE = 1\r\n","\r\n","def compute_batch_iou(dataset, image_ids, verbose=1):\r\n","    IoUs = []\r\n","    for image_id in image_ids:\r\n","        # Load image\r\n","        image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\r\n","            modellib.load_image_gt(dataset, config,\r\n","                                   image_id, use_mini_mask=False)\r\n","        # Run object detection\r\n","        #results = model.detect_molded(image[np.newaxis], image_meta[np.newaxis], verbose=0)\r\n","        results = model.detect([image], verbose=0)\r\n","        # Compute Iou over range 0.5 to 0.95\r\n","        r = results[0]\r\n","\r\n","\r\n","\r\n","\r\n","        iou = utils.compute_overlaps_masks(gt_mask, r['masks'])\r\n","        IoUs.append(iou)\r\n","        \r\n","    return IoUs\r\n","\r\n","\r\n","\r\n","def compute_final_iou(iou_list):\r\n","# compute_overlaps_masks() takes two arguments: mask1 and mask2. \r\n","# Assume mask1 has m instances, and mask2 has n instances, \r\n","# the function return an m*n array A. A[i, j] represents the IoU of ith instances of mask1 and jth instances of mask2.\r\n","    np.set_printoptions(suppress=True)\r\n","\r\n","    res = []\r\n","    mydict = {}\r\n","\r\n","    #iterate over true masks\r\n","    for i in range(len(iou_list)): # number of pictures\r\n","    # iterate over predicte masks\r\n","        res = []\r\n","        for j  in range(len(iou_list[i])): # true masks 16\r\n","            \r\n","            mask_max =iou_list[i][j].max() # maximum match in mask\r\n","            # all max matches of true masks\r\n","            res.append(mask_max)\r\n","            \r\n","            # save image ID (i) and result of IoU\r\n","        mydict[i] = res\r\n","\r\n","    my_dictionary = {k: round(statistics.mean(v),3) for k, v in mydict.items()}\r\n","    \r\n","    return my_dictionary\r\n","\r\n","def compute_leaf_error(IoUs):\r\n","    \"\"\" computes the percentage of change between true masks and\r\n","        predicted masks\"\"\"\r\n","    mydict = {}\r\n","    #iterate over true masks\r\n","    for i in range(len(IoUs)): # number of pictures\r\n","        error = round((IoUs[i].shape[0] - IoUs[i].shape[1]) / IoUs[i].shape[1], 3)\r\n","        mydict[i] = error\r\n","\r\n","    return mydict\r\n","\r\n","\r\n","\r\n","# run \r\n","ious = compute_batch_iou(dataset, image_ids)\r\n","ious_perc = compute_final_iou(ious)\r\n","leaf_error = compute_leaf_error(ious)\r\n","\r\n","\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LW45njm11QLf","executionInfo":{"status":"ok","timestamp":1609967742348,"user_tz":-60,"elapsed":1415,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"67a52fb6-3fd6-463f-d40c-31d98e69b1ac"},"source":["print(\"the leaf error per image are:\", leaf_error)\r\n","\r\n","count = 0\r\n","_sum = 0\r\n","for key in leaf_error:\r\n","    count += 1\r\n","    _sum += leaf_error[key]\r\n","\r\n","print('this is the mean: ', _sum/count)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["the leaf error per image are: {0: -0.059, 1: -0.133, 2: 0.083, 3: 0.125, 4: -0.067, 5: -0.071, 6: 0.0, 7: -0.105, 8: 0.214, 9: 0.25, 10: -0.222}\n","this is the mean:  0.0013636363636363648\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KtJpC8bRkykf","executionInfo":{"status":"ok","timestamp":1609967563920,"user_tz":-60,"elapsed":79945,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"564de80b-f36d-409d-d234-85cb538166ec"},"source":["print(\"the ious per image are:\", ious_perc)\r\n","\r\n","count = 0\r\n","_sum = 0\r\n","for key in ious_perc:\r\n","    count += 1\r\n","    _sum += ious_perc[key]\r\n","\r\n","print('this is the mean: ', _sum/count)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["the ious per image are: {0: 0.744, 1: 0.818, 2: 0.52, 3: 0.775, 4: 0.845, 5: 0.565, 6: 0.754, 7: 0.616, 8: 0.652, 9: 0.591, 10: 0.781}\n","this is the mean:  0.6964545520869169\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"h92sATY8JXjF"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hHU6wH2YZhHK"},"source":["import json\r\n","\r\n","# save iou\r\n","with open(f'DEFAULT_LOGS_DIR/results_test/{current_model}_iou.json', 'w') as fp:\r\n","    json.dump(str(ious_perc), fp)\r\n","\r\n","# save iou\r\n","with open(f'DEFAULT_LOGS_DIR/results_test/{current_model}_error.json', 'w') as fp:\r\n","    json.dump(str(leaf_error), fp)\r\n","\r\n","\r\n","# # load json\r\n","# with open('DEFAULT_LOGS_DIR/model_1.json', 'r') as fp:\r\n","#     data = json.load(fp)\r\n","\r\n","# convert strin back to dict\r\n","# data1 = eval(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vPy_tCp5M-KW","executionInfo":{"status":"ok","timestamp":1609967563923,"user_tz":-60,"elapsed":79927,"user":{"displayName":"Kail Kuhn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVJqby0gfySeM6lEtPlBUCStXfVG9P_xVNXsYmDQ=s64","userId":"17980787070580531145"}},"outputId":"95a80d1b-4cff-49dd-fda1-505b74aa424e"},"source":["current_model"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'model_9'"]},"metadata":{"tags":[]},"execution_count":19}]}]}